---
title: "**projpred**: Projection predictive feature selection"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
params:
  EVAL: !r identical(Sys.getenv("NOT_CRAN"), "true")
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{projpred: Projection predictive feature selection}
  \usepackage[utf8]{inputenc}
---

```{r, child="children/SETTINGS-knitr.txt"}
```

This vignette shows the main functionalities of the **projpred** package, which implements the projective variable selection for generalized linear models as well as generalized linear and additive multilevel models. The specialty of the projective variable selection is that it not only performs a variable selection, but that it also provides valid post-selection inference.

The projective variable selection is based on the ideas of Goutis and Robert (1998) and Dupuis and Robert (2003). The methods implemented in **projpred** are described in detail in Piironen et al. (2020) and Catalina et al. (2020). They are evaluated in comparison to many other methods in Piironen and Vehtari (2017a). Type `citation("projpred")` for details on how to cite **projpred**.

## Data

For this vignette, we borrow the `CO2` dataset from the **datasets** package (which should have been shipped with your original R installation). It contains 84 observations from an experiment on the cold tolerance of a specific grass species (see `?CO2`). The experiment has been carried out on 12 `Plant`s, with 7 observations per plant. Each plant belongs to one of two `Type`s of origin (half of the plants are from Quebec, the other half from Mississippi). Within each type of origin, the same number of plants has undergone a special `Treatment` (chilling) before measuring the response, the carbon dioxide `uptake` of the plant. The ambient carbon dioxide `conc`entration has also been measured.
```{r}
# Should not be necessary since the 'datasets' package should have been shipped
# with your original R installation, but stated here for a better traceability
# of our steps:
data("CO2", package = "datasets")
```
**TODO: All wording referring to the former dataset**

Note that this is not a typical dataset for a variable selection problem since it involves only few predictors (even when counting their interactions and setting this in relation to the number of observations). We'll use it for demonstrative purposes here.

## Reference model

First, we have to construct the reference model for the projective variable selection. This model is considered as the best ("reference") solution to the prediction task and the aim of the projective variable selection is to find a subset of a set of candidate predictors which is as small as possible but achieves a predictive performance as close as possible to that of the reference model.

The **projpred** package is compatible with reference models fit by the **rstanarm** and **brms** packages. To our knowledge, **rstanarm** and **brms** are currently the only packages for which `get_refmodel()` methods (which establish the compatibility with **projpred**) exist. Custom reference models can also be used via `init_refmodel()`, as shown in section "Examples" in the `?init_refmodel` help.^[We will cover custom reference models more deeply in a future vignette.] For both, **rstanarm** and **brms** reference models, **projpred** will consider all predictors from the reference model as candidate predictors, so all candidate models are submodels of the reference model.^[In principle, this is not a necessary assumption for a projective variable selection (see, e.g., Piironen et al., 2020) and custom reference models allow to avoid this assumption, but for **rstanarm** and **brms** reference models, this is a reasonable assumption which simplifies implementation a lot.]

Here, we will use the **rstanarm** package to fit the reference model. If you want to use the **brms** package, you simply have to replace the **rstanarm** fit (of class `stanreg`) in the code below by your **brms** fit (of class `brmsfit`).

For our **rstanarm** reference model, we use the Gaussian distribution as the `family` for our response, `uptake`.^[Currently, the families supported by **projpred** are `gaussian()`, `binomial()` (and, via `brms::get_refmodel.brmsfit()`, `brms::bernoulli()`), and `poisson()`.] With regard to the predictors, we construct the most comprehensive model which is possible from the predictor variables listed above [**TODO** really?]. `Plant` constitutes the typical case where partially pooled (also known as *group-level* or---in frequentist terms---*random*) effects are desirable, so we will use a multilevel formula term for `Plant`.

We use **rstanarm**'s default priors, except for the regression coefficients of the predictors with non-pooled (also known as *population-level* or *fixed*) effects for which we use a regularized horseshoe (RH) prior (Piironen and Vehtari, 2017c) with the hyperprior for the its global shrinkage parameter being based on Piironen and Vehtari (2017b,c). Before applying the RH prior, we standardize the predictor variables (Piironen and Vehtari, 2017c). Note that the `CO2` dataset is not a typical one for the RH prior. Just like for variable selection problems in general (see above), datasets to which the RH prior is applied usually involve many more predictors, especially when setting the number of predictors in relation to the number of observations. We'll use the RH prior here anyway, but only for demonstrative purposes. In R code, these are the preparation steps for the RH prior:
```{r}
# Number of regression coefficients (for `zType * zconc * zTreatment`):
( D <- sum(sapply(seq_len(3), choose, n = 3)) )
```

```{r}
# Prior guess for the number of relevant (i.e., non-zero) regression
# coefficients (in this example, this is just a heuristic guess and might not
# make sense to biologists):
p0 <- 4
# Number of observations:
N <- nrow(CO2)
# Hyperprior scale for tau, the global shrinkage parameter (note that for the
# Gaussian family, 'rstanarm' will automatically scale this by the residual
# standard deviation):
tau0 <- p0 / (D - p0) * 1 / sqrt(N)
# Standardize the predictor variables:
CO2 <- within(CO2, {
  zType <- as.vector(scale(as.numeric(Type) - 1))
  zTreatment <- as.vector(scale(as.numeric(Treatment) - 1))
  zconc <- as.vector(scale(conc))
})
```

We now fit the reference model to the data. Note that for multilevel formula terms, **projpred** uses the **lme4** syntax. To make this vignette build fast, we use only 2 Markov chains and 500 iterations per chain (with half of them being discarded as warmup draws). In practice, 4 chains and 2000 iterations per chain are reasonable defaults. If sampling for the reference model still takes long and multiple CPU cores are available, uncomment the line setting option `mc.cores` to an automatically detected number of cores (or set `mc.cores` to a manually determined number of cores).
```{r}
library(rstanarm)
# options(mc.cores = parallel::detectCores())
refm_fit <- stan_glmer(uptake ~ zType * zconc * zTreatment + (1 | Plant),
                       family = gaussian(),
                       data = CO2,
                       prior = hs(global_scale = tau0),
                       QR = TRUE, chains = 2, iter = 500, seed = 2052109,
                       refresh = 0)
```
Usually, we would now have to check the convergence diagnostics (see, e.g., `?posterior::diagnostics` and `?posterior::default_convergence_measures`). However, due to the technical reasons for which we reduced `chains` and `iter`, we will skip this step here.

## Variable selection

Now, **projpred** comes into play.
```{r}
library(projpred)
```

In **projpred**, the projective variable selection consists of a *search* and an *evaluation*. The search determines the solution path, i.e., the best submodel for each number of predictor terms (model size) or, in other words, the predictor terms ordered by predictive relevance (with the most predictive predictor term at the beginning). The evaluation determines the predictive performance of the submodels along the solution path.

There are two functions for performing the variable selection: `varsel()` and `cv_varsel()`. In contrast to `varsel()`, `cv_varsel()` performs a cross-validation (CV) by running the search with the training data of each CV fold separately (an exception is explained in the `?cv_varsel` help) and running the evaluation on the corresponding test set of each CV fold. Because of this CV, `cv_varsel()` is recommended over `varsel()`. Thus, we'll use `cv_varsel()` here.

It is worth taking a look at the documentation of `cv_varsel()` (see `?cv_varsel`) because we cannot explain all arguments here. Note that we'll modify some arguments here and later in this vignette only to speed up the building of this vignette. This is not recommended in general. For example, we modify argument `cv_method` to use a K-fold CV instead of a leave-one-out (LOO) CV. In general, the LOO CV is more accurate. In this example, the two `cv_method`s give very similar results.
```{r, results='hide'}
cvvs <- cv_varsel(
  refm_fit,
  ### Only for the sake of speed (in general, LOO CV is more accurate):
  cv_method = "kfold",
  ###
  ### CAUTION: Do not use this for final results in practice:
  ndraws = 5,
  ndraws_pred = 100,
  ###
  seed = 411183
)
```

The first step after running the variable should be the decision for a model size (i.e., a number of predictor terms to include in the final model). This should be the first step (in particular, before inspecting the solution path) in order to avoid user-induced selection bias (which could occur if the user made the model size decision dependent on the solution path). To decide for a model size, there are several statistics we can plot as a function of the model size. Here, we'll use the (estimated) expected log predictive density (ELPD; empirically, this is the sum of the log predictive densities of the observations in the evaluation---or "test"---set) and the root mean squared error (RMSE). By default, the statistics are shown on their original scale, but with `deltas = TRUE`, the statistics are shown as differences from a baseline model (which is the reference model by default, at least in the most common case). Since the differences are usually of more interest (at least in the model size decision), we'll directly plot with `deltas = TRUE` here:
```{r, fig.asp = 1.5 * (sqrt(5) - 1) / 2}
plot(cvvs, stats = c("elpd", "rmse"), deltas = TRUE)
```
Based on that plot, we would decide for a model size of 2 because that's the point where the performance measures level off.
```{r}
modsize_decided <- 2
```

Note that **projpred** includes a function to help deciding for a model size, but this is a rather heuristic decision and needs to be used with caution (see the `?suggest_size` help):
```{r, eval=FALSE}
suggest_size(cvvs)
```
Here, we would get the same result (`2`) as by our manual decision.

Only now, after we have made a decision for the model size, we inspect the further results from the variable selection and, in particular, the solution path. For example, to get some basic results, we simply `print()` the resulting object:
```{r}
cvvs
### Alternative modifying the number of printed decimal places:
# print(cvvs, digits = 2)
### 
```
The solution path can be seen in the `print()` output, but it is also accessible through the `solution_terms()` function:
```{r}
( soltrms <- solution_terms(cvvs) )
```

Combining the decided model size of 2 with the solution path leads to `(1 | Plant)` and `zconc` (as well as the intercept) as the predictor terms of the final submodel:
```{r}
( soltrms_final <- head(soltrms, modsize_decided) )
```

## Post-selection inference

The `project()` function returns an object of class `projection` which forms the basis for convenient post-selection inference. By the following code, `project()` will project the reference model onto the final submodel once again:^[During the forward search, the reference model has already been projected onto all candidate models (this was where arguments `ndraws` and `nclusters` of `cv_varsel()` came into play). During the evaluation of the submodels along the solution path, the reference model has already been projected onto those submodels (this was where arguments `ndraws_pred` and `nclusters_pred` of `cv_varsel()` came into play). In principle, one could use the results from the evaluation step for post-selection inference, but due to a bug in the current implementation (see GitHub issue #168), we currently have to project once again.]
```{r}
prj <- project(
  refm_fit,
  solution_terms = soltrms_final,
  ### CAUTION: Do not use this for final results in practice:
  ndraws = 100,
  ###
  seed = 15705533
)
```
<!-- Alternative, as soon as GitHub issue #168 is resolved: -->
<!-- ```{r} -->
<!-- prj <- project( -->
<!--   cvvs, -->
<!--   nterms = modsize_decided, -->
<!--   cv_search = FALSE, -->
<!--   seed = 15705533 -->
<!-- ) -->
<!-- ``` -->
For more accurate results, we could have increased argument `ndraws` of `project()` (up to the number of posterior draws in the reference model). This increases runtime, though, which we don't want in this vignette (for technical reasons).

Next, we create a matrix containing the projected posterior draws stored in the depths of `project()`'s output object of class `projection`:
```{r}
prj_mat <- as.matrix(prj)
```
This matrix is all we need for post-selection inference. It can be used like any matrix of draws from Markov chain Monte Carlo (MCMC) procedures, except that it doesn't reflect a typical posterior distribution, but rather a projected posterior distribution, i.e. the distribution arising from the deterministic projection of the reference model's posterior distribution onto the parameter space of the final submodel.

### Marginals of the projected posterior

The **posterior** package provides a general way to deal with posterior distributions, so it can also be applied to our projected posterior. For example, to calculate summary statistics for the marginals of the projected posterior:
```{r}
library(posterior)
prj_drws <- as_draws_matrix(prj_mat)
summarize_draws(
  prj_drws,
  "median", "mad", function(x) quantile(x, probs = c(0.025, 0.975))
)
```

A visualization of the projected posterior can be achieved with the **bayesplot** package, for example using its `mcmc_intervals()` function. To improve the readability of that plot, we exclude the overall intercept as well as the group-level intercepts.
```{r}
library(bayesplot)
bayesplot_theme_set(ggplot2::theme_gray())
# Negative lookaheads are not possible in argument `regex_pars` of
# mcmc_intervals(), so we use the following workaround:
pars_nms_prj <- grep("^b_Intercept", colnames(prj_mat), value = TRUE,
                     invert = TRUE)
pars_nms_prj <- grep("^[^r]", pars_nms_prj, value = TRUE)
mcmc_intervals(prj_mat, pars = pars_nms_prj) +
  ggplot2::coord_cartesian(xlim = c(0, 13))
```
Note that we only visualize the *1-dimensional* marginals of the projected posterior here. To gain a more complete picture, we would have to visualize *2-dimensional* marginals of the projected posterior (i.e., marginals of pairs of parameters), giving 3-dimensional plots.

For comparison, consider the marginal posteriors of the corresponding parameters from the reference model:
```{r}
refm_mat <- as.matrix(refm_fit)
# See `colnames(refm_mat)` for possible parameter names usable in
# mcmc_intervals().

# Multilevel terms have to be treated specially (note that the following code
# tries to be as general as possible, but since it also has to be as concise as
# possible, it only makes sense for a single term with "random intercepts"):
pars_nms_pp <- grep("^\\(.*\\|.*\\)$", soltrms_final, value = TRUE)
pars_nms_pp <- sub("^.*\\|[[:blank:]]*(.+)[[:blank:]]*\\)$", "\\1", pars_nms_pp)
pars_nms_pp <- paste0("Sigma[", pars_nms_pp, ":(Intercept),(Intercept)]")
pars_nms_np <- grep("^\\(.*\\|.*\\)$", soltrms_final, value = TRUE,
                    invert = TRUE)
pars_nms <- c(pars_nms_np, pars_nms_pp, "sigma")
mcmc_intervals(refm_mat, pars = pars_nms) +
  ggplot2::coord_cartesian(xlim = c(0, 13))
```

### Predictions

Predictions from the final submodel can be made by `proj_linpred()` and `proj_predict()`. For example, suppose we have the following new observations:
```{r}
CO2_new <- data.frame(
  Plant = c("Qn4", "Qc4", "Mn4"),
  conc = c(300, 800, 500)
)
CO2_new$zconc <- (CO2_new$conc - mean(CO2$conc)) / sd(CO2$conc)
```

We'll start with `proj_linpred()` which calculates the linear predictor^[It can also transform the linear predictor to response scale, but because of the identity link function we are using here, this is the same as the linear predictor scale.] for all new observations and, depending on argument `integrated`, averaged or not averaged across the projected draws. If `CO2_new` also contained response values (i.e., `uptake` values) for its new observations, then `proj_linpred()` would also evaluate the log predictive density at these. For instance, the following computes the expected values of the new observations' predictive distributions^[Beware that this statement is correct here because of the Gaussian family with the identity link function. For other families (which usually come in combination with a different link function), one would typically have to use `transform = TRUE` in order to make this statement correct.]:
```{r}
prj_linpred <- proj_linpred(prj, newdata = CO2_new, integrated = TRUE)
cbind(CO2_new, linpred = as.vector(prj_linpred$pred))
```

With `proj_predict()`, we can obtain draws from the final submodel's predictive distribution. In contrast to `proj_linpred(<...>, integrated = FALSE)`, this contains not only the uncertainty arising from parameter estimation, but also the uncertainty arising from the observational (or "sampling") model for the response (in case of the Gaussian family we are using here, this is the uncertainty arising from the residual standard deviation). This is useful for what is usually termed a posterior predictive check (PPC), but would have to be termed a projected posterior predictive check (or similarly) here.
```{r}
prj_predict <- proj_predict(prj, .seed = 762805)
# Using the 'bayesplot' package:
bayesplot_theme_set(ggplot2::theme_bw())
ppc_dens_overlay(y = CO2$uptake, yrep = prj_predict, alpha = 0.9, bw = "SJ")
```
This projected posterior predictive check indicates that there is clearly room for improvement of the model (because the observed response values exhibit a bimodal distribution whereas the replicated response values don't).

## References

Catalina, A., Bürkner, P.-C., and Vehtari, A. (2020). Projection predictive inference for generalized linear and additive multilevel models. *arxiv:2010.06994*. URL: <https://arxiv.org/abs/2010.06994>.

Dupuis, J. A. and Robert, C. P. (2003). Variable selection in qualitative models via an entropic explanatory power. *Journal of Statistical Planning and Inference*, **111**(1-2):77–94. doi:[10.1016/S0378-3758(02)00286-0](https://doi.org/10.1016/S0378-3758(02)00286-0).

Goutis, C. and Robert, C. P. (1998). Model choice in generalised linear models: A Bayesian approach via Kullback–Leibler projections. *Biometrika*, **85**(1):29–37.

Piironen, J. and Vehtari, A. (2017a). Comparison of Bayesian predictive methods for model selection. *Statistics and Computing*, **27**(3):711-735. doi:[10.1007/s11222-016-9649-y](https://doi.org/10.1007/s11222-016-9649-y).

Piironen, J. and Vehtari, A. (2017b). On the hyperprior choice for the global shrinkage parameter in the horseshoe prior. In *Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS)*, PMLR 54:905-913, 2017. URL: [http://proceedings.mlr.press/v54/piironen17a.html](http://proceedings.mlr.press/v54/piironen17a.html).

Piironen, J. and Vehtari, A. (2017c). Sparsity information and regularization in the horseshoe and other shrinkage priors. *Electronic Journal of Statistics*, **11**(2): 5018-5051. doi:[10.1214/17-EJS1337SI](https://doi.org/10.1214/17-EJS1337SI).

Piironen, J., Paasiniemi, M., and Vehtari, A. (2020). Projective inference in high-dimensional problems: Prediction and feature selection. *Electronic Journal of Statistics*, **14**(1):2155-2197. doi:[10.1214/20-EJS1711](https://doi.org/10.1214/20-EJS1711).
