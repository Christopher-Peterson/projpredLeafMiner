---
title: "Latent projection predictive feature selection"
date: "`r Sys.Date()`"
bibliography: references.bib
link-citations: true
output:
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 4
params:
  EVAL: !r identical(Sys.getenv("NOT_CRAN"), "true")
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Latent projection predictive feature selection}
  %\VignetteEncoding{UTF-8}
---

```{r child="children/SETTINGS-knitr.txt"}
```

## Introduction

This vignette shows how to apply the latent projection predictive feature selection from @catalina_latent_2021 using **projpred**.

```{r}
# Attach packages that are needed for this vignette:
library(brms)
library(projpred)
# Set this manually if desired:
ncores <- parallel::detectCores(logical = FALSE)
### Only for technical reasons in this vignette (you can omit this when running
### the code yourself):
ncores <- min(ncores, 2L)
###
options(mc.cores = ncores)
```

```{r, echo=FALSE}
# Building the vignette fails in `R CMD check` if we use the rstan backend, so
# use the cmdstanr backend:
options(brms.backend = "cmdstanr")
```

## General idea

For generalised linear models whose variate's distribution does not belong to the exponential family, the KL divergence minimisation problem solved in projection is not analytically easy to compute in general. This is the foundational idea of efficient projection predictive inference. In order to bypass this issue, we solve the KL minimisation in the latent predictive space $p(\tilde{\eta}|\lambda_*)$ under the general model formulation for data $\mathcal{D}=\{\boldsymbol X | y\}$
$$
y\sim p(\mu, \phi),\quad\mu = g(\eta),\quad \eta\sim p(\lambda|\boldsymbol X),
$$
where we have regressions parameters $\lambda$, inverse link function $g(\cdot)$, and $\eta$ is called the latent predictor.

We make the assumption in this setting that the latent predictor has a Gaussian distribution, since it is the linear combination of our data and our regressors and has infinite support. In the case where $\eta$ has two finite moments, then the Gaussian distribution has the highest differential entropy over all distributions with two finite moments and infinite support [see, e.g., @cover_elements_1991]. In doing so, we return to the "easy" exponential family case, and can implement the original `projpred` workflow once more.

<!-- ## Exponential GLM example -->

<!-- In the case of an exponential variate distribution, it has been shown in @catalina_latent_2021 this Gaussian approximation can improve training times and occasionally identify superior search paths to vanilla `projpred`. -->

<!-- To begin with, we generate some fake Poisson GLM data. -->

<!-- ```{r} -->
<!-- # generate random data -->
<!-- generate_fake_poisson_data <- -->
<!--   function(N = 100, -->
<!--            G = round(N / 10), -->
<!--            P = 3) { -->
<!--     # regression coefficients -->
<!--     beta <- rnorm(P) -->
<!--     # sampled covariates, group means and fake data -->
<!--     fake <- matrix(rnorm(N * P), ncol = P) -->
<!--     dimnames(fake) <- list(NULL, paste0("x", 1:P)) -->
<!--     # fixed effect part and sampled group membership -->
<!--     fake <- transform(as.data.frame(fake), -->
<!--                       theta = fake %*% beta, -->
<!--                       g = sample.int(G, N, replace = TRUE)) -->
<!--     # add random intercept by group -->
<!--     fake  <- -->
<!--       merge(fake, data.frame(g = 1:G, eta = rnorm(G)), by = "g") -->
<!--     # linear predictor -->
<!--     fake  <- transform(fake, mu = theta + eta) -->
<!--     # sample Poisson data -->
<!--     fake  <- transform(fake, y = rpois(N, exp(mu))) -->
<!--     # shuffle order of data rows to ensure even distribution of computational effort -->
<!--     fake <- fake[sample.int(N, N), ] -->
<!--     # drop not needed row names -->
<!--     rownames(fake) <- NULL -->
<!--     return(fake) -->
<!--   } -->
<!-- set.seed(300416) -->
<!-- pois <- generate_fake_poisson_data() -->
<!-- ``` -->

<!-- We begin by fitting some reference model we believe to be a good fit to the data in BRMS. -->

<!-- ```{r} -->
<!-- # build reference model -->
<!-- model_poisson <- brm( -->
<!--   formula = y ~ 1 + x1 + x2, -->
<!--   data = pois, -->
<!--   family = poisson(), -->
<!--   prior = set_prior("normal(0, 1)", class = "b"), -->
<!--   seed = 300416, refresh = 0 -->
<!-- ) -->
<!-- ``` -->

<!-- We now extract a reference model in terms of the latent predictor under the Gaussianity assumption. This is done in `projpred` by setting `latent = TRUE`. -->

<!-- ```{r} -->
<!-- # define new reference model for latent predictor -->
<!-- latent_ref <- get_refmodel( -->
<!--   model_poisson, -->
<!--   latent = TRUE -->
<!-- ) -->
<!-- ``` -->

<!-- Now having a reference model where the variate's distribution belongs to the exponential family (in this case it is Gaussian), we are able to employ the standard `varsel` function from `projpred` to perform variable selection. The results are then plotted below. -->

<!-- ```{r} -->
<!-- # perform variable selection on the latent reference model -->
<!-- latent_vs <- varsel(latent_ref, seed = 95930) -->
<!-- # selection order of the variables -->
<!-- solution_terms(latent_vs) -->
<!-- # plot varsel results -->
<!-- plot(latent_vs, stats = c('elpd', 'rmse'), seed = 4537) -->
<!-- ``` -->

<!-- We can compare this implementation of latent projection predictive variable selection to vanilla `projpred` since the underlying covariate does in fact have distribution belonging to the exponential family. This is done below. -->

<!-- ```{r} -->
<!-- # get reference model from BRMS object -->
<!-- refmodel <- get_refmodel(model_poisson) -->
<!-- # perform variable selection -->
<!-- vanilla_vs <- varsel(refmodel, seed = 95930) -->
<!-- # selection order of the variables -->
<!-- solution_terms(vanilla_vs) -->
<!-- # plot results -->
<!-- plot(vanilla_vs, stats = c('elpd', 'rmse'), seed = 4537) -->
<!-- ``` -->

## References
